{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe49011-c8b3-40d4-adcb-4cc86913681a",
   "metadata": {},
   "source": [
    "# Hamilton Character Analysis Project\n",
    "A wip little NLP project looking at the speech of each character in Hamilton :3 \n",
    "\n",
    "## 1. Extracting character lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "365a286f-60d8-4857-a392-a02cd6cc20f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "content = \"\"\n",
    "with open(\"act1.txt\", encoding=\"utf-8\") as file:\n",
    "    content += file.read()\n",
    "with open(\"act2.txt\", encoding=\"utf-8\") as file:\n",
    "    content += file.read()\n",
    "    content = re.sub(r'\\n\\d+\\s*\\n', '\\n', content)\n",
    "with open(\"raw.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1670f2e9-4cf2-4ce2-b39e-e4980f795c96",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e9dd6438-55eb-4884-9650-cb5a76d11b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\grace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\grace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\grace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')  # For lemmatization\n",
    "# nltk.download('punkt_tab') # This downloads the punkt tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d60a4729-696a-44b2-aca2-49112db8ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_lines(character):\n",
    "    \"\"\"\n",
    "    extracts all lines(string) from character(string).\n",
    "    \"\"\"\n",
    "    content = \"\"\n",
    "    with open(\"raw.txt\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # ([A-Z]+(?:\\s+[A-Z]+)* \\n)\n",
    "    # for extracting all singer names (bug with \"FULL COMPANY (EXCEPT HAMILTON) \")\n",
    "    content = re.sub(r\"\\d+\\..+\\n\", \"\", content)  # stripping song names\n",
    "    regex = character.upper() + r\".*([\\s\\S]+?)(?=\\b[A-Z]{2,}\\b)\"\n",
    "    lines = re.findall(regex, content)\n",
    "    result = []\n",
    "    for index, line in enumerate(lines):\n",
    "        line = line.replace(\"\\\\n\", \" \").replace(\"/\", \"\").strip()\n",
    "        line = line.replace('\\n', '').strip()\n",
    "        line = re.sub(r\"[â€™']\", \"\", line)\n",
    "        result.append(line)\n",
    "    all_lines = \" \".join(result)\n",
    "    return all_lines\n",
    "    print(get_lines(\"eliza\")[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168ed9ed-13c1-4466-a4b0-d0cdb6370d9e",
   "metadata": {},
   "source": [
    "## Preprocess text\n",
    "\n",
    "* tokenize\n",
    "* removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c1f3ca4-12d6-4409-996b-6ec108b10390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered more: ['hear', 'ye', 'hear', 'ye', 'name', 'samuel', 'seabury', 'present', 'free', 'thoughts', 'proceedings', 'continental', 'congress', 'heed', 'not', 'rabble', 'scream', 'revolution', 'not', 'interests', 'heart', 'chaos', 'bloodshed', 'not', 'solution', 'dont', 'let', 'lead', 'astray', 'congress', 'not', 'speak', 'theyre', 'playing', 'dangerous', 'game', 'pray', 'king', 'shows', 'mercy', 'shame', 'heed', 'not', 'rabble', 'scream', 'revolution', 'not', 'interests', 'heart', 'chaos', 'bloodshed', 'not', 'solution', 'dont', 'let', 'lead', 'astray', 'congress', 'not', 'speak', 'theyre', 'playing', 'dangerous', 'game', 'pray', 'king', 'shows', 'mercy', 'shame', 'shame', 'not', 'cousin', 'cousin', 'committed', 'suicide', 'left', 'nothin', 'ruined', 'pride', 'something', 'new', 'inside', 'voice', 'saying', 'got', 'ta', 'fend', 'started', 'retreatin', 'readin', 'every', 'treatise', 'shelf', 'trusted', 'outgunned', 'outmanned', 'outnumbered', 'outplanned', 'got', 'ta', 'make', 'stand', 'ayo', 'im', 'gon', 'na', 'need', 'man', 'check', 'real', 'second', 'millisecond', 'let', 'guard', 'tell', 'people', 'feel', 'second', 'im', 'model', 'modern', 'major', 'general', 'venerated', 'virginian', 'veteran', 'whose', 'men', 'lining', 'put', 'pedestal', 'writin', 'letters', 'relatives', 'embellishin', 'elegance', 'eloquence', 'elephant', 'room', 'truth', 'ya', 'face', 'ya', 'hear', 'british', 'cannons', 'hope', 'success', 'fleeting', 'keep', 'leading', 'people', 'im', 'leading', 'keep', 'retreating', 'put', 'stop', 'bleeding', 'british', 'take', 'brooklyn', 'knight', 'takes', 'rook', 'look', 'outgunned', 'outmanned', 'outnumbered', 'outplanned', 'got', 'ta', 'make', 'stand', 'ayo', 'im', 'gon', 'na', 'need', 'man', 'incoming', 'goes', 'cannon', 'watch', 'blood', 'shit', 'spray', 'goes', 'cannon', 'abandonin', 'kips', 'bay', 'theres', 'another', 'ship', 'lost', 'southern', 'tip', 'got', 'ta'] \n",
      "\n",
      "filtered_out : {'only', 'are', 'now', 'between', 'because', 'through', 'few', 'own', 'against', 'most', 'same', 'once', 'above', 'any', 'down', 'am', 'my', 'when', 'themselves', 'him', 'no', 'has', 'it', 'yourself', 'an', 'but', 'its', 'me', 'your', 'out', 'than', 'too', 'some', 'she', 'other', 'himself', 'again', 'he', 'while', 'can', 'which', 'up', 'to', 'had', 'before', 'there', 'our', 'for', 'each', 'won', 'these', 'here', 'after', 'you', 'further', 'very', 'them', 'on', 'was', 'a', 'did', 'they', 'this', 'if', 'myself', 'will', 'in', 'about', 'the', 'his', 'just', 'yours', 'more', 'their', 'with', 'doing', 'into', 'o', 'such', 'and', 'been', 'then', 'have', 'do', 'as', 'what', 'from', 'being', 'why', 'should', 'by', 'over', 'where', 'her', 'those', 'be', 'we', 'were', 'who', 'that', 'i', 'is', 'does', 'both', 'how', 'so', 'under', 'or', 'of', 'having', 'until', 'off', 'at', 'all'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def tokenize_lines(line):\n",
    "    # tokenize and strip punctuation\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "    return tokens\n",
    "    \n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    important_words = {'not'}\n",
    "    \n",
    "    custom_stopwords = stop_words - important_words\n",
    "    result = []\n",
    "    removed = set()\n",
    "    for token in tokens:\n",
    "        if token in custom_stopwords:\n",
    "            removed.add(token)\n",
    "        else:\n",
    "            result.append(token)\n",
    "    return (result, removed)\n",
    "    # print(f\"removed: {removed}\")\n",
    "    # print(f\"final result: {result}\")\n",
    "    \n",
    "all_lines = \"\"\n",
    "for char in characters:\n",
    "    all_lines = all_lines + get_lines(char)\n",
    "    \n",
    "tokens = tokenize_lines(all_lines)\n",
    "# print(f\"tokenize_lines words: {tokens[:200]} \\n\")\n",
    "\n",
    "(filtered, filtered_out) = remove_stopwords(tokens)\n",
    "print(f\"filtered more: {filtered[:200]} \\n\")\n",
    "print(f\"filtered_out : {filtered_out} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a645f865-12a6-4623-9db9-a0dd87f4b62f",
   "metadata": {},
   "source": [
    "## Saving the character lines in the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "891658b5-4745-497f-97ad-3b28025ec296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "characters = [\"SEABURY\", \"WASHINGTON\", \"MADISON\", \"LAURENS\", \"ELIZA\", \"MARIA\", \n",
    "              \"HAMILTON\", \"LAFAYETTE\", \"MULLIGAN\", \"JEFFERSON\", \"PHILIP\", \"BURR\",\n",
    "              \"ANGELICA\", \"PEGGY\"]\n",
    "\n",
    "character_lines = []\n",
    "for c in characters:\n",
    "    character_map = {}\n",
    "    character_map[\"character\"] = c.lower()\n",
    "    \n",
    "    lines = get_lines(c)\n",
    "    character_map[\"lines\"] = get_lines(c)\n",
    "    \n",
    "    tokens = tokenize_lines(get_lines(c))\n",
    "\n",
    "    # print(\"\\n\\n\" + c + \"\\n\" + str(tokens))\n",
    "    character_map[\"tokens\"] = \" \".join(tokens)\n",
    "\n",
    "    tokens, _ = remove_stopwords(tokens)\n",
    "    # print(\"\\n\\n\" + c + \"\\n\" + str(tokens))\n",
    "    character_map[\"filtered_tokens\"] = \" \".join(tokens)\n",
    "    \n",
    "    character_lines.append(character_map)\n",
    "    \n",
    "df = pd.DataFrame(character_lines)\n",
    "df.to_csv(\"character_lines.csv\", index=False)  # index=False skips the row numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4920ffe-9e26-4bc3-abeb-9570bf97737c",
   "metadata": {},
   "source": [
    "# Getting Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d41263-f940-4142-ab82-4cc189b57e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "6fdffd72-9d07-4e65-8d5d-4cb1008e353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247f1e0-49f0-45d3-bb87-388e0095f1ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# ðŸŽ­ Hamilton Lyrics Analysis Project Roadmap\n",
    "\n",
    "This project explores character identity and style in *Hamilton* using NLP and machine learning. There are two main parts:\n",
    "\n",
    "- **Part 1:** Compare and analyze character lyrics (semantic, stylistic, emotional).\n",
    "- **Part 2:** Build an ML model to predict who said a given line.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Part 1: Character Lyrics Analysis\n",
    "\n",
    "### 1. Extract and Clean Character Lines\n",
    "- [ ] Parse the script using regex to assign lines to characters.\n",
    "- [ ] Normalize contractions (e.g. â€œIâ€™mâ€ â†’ â€œI amâ€) and remove stage directions.\n",
    "- [ ] Filter down to major characters (e.g. HAMILTON, BURR, ANGELICA, ELIZA, etc.)\n",
    "\n",
    "> **Why it's interesting:** Creates a solid foundation for every analysis step. Also fun to debug regex against a real-world script format!\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Bag-of-Words + TF-IDF Vectors\n",
    "- [ ] Use `TfidfVectorizer` to convert character documents into vectors.\n",
    "- [ ] Compute cosine similarity between characters.\n",
    "- [ ] Visualize as a similarity matrix or heatmap.\n",
    "\n",
    "> **Why it's interesting:** Shows how â€œcloseâ€ characters are based on word usage â€” maybe Burr and Jefferson cluster together?\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Topic Modeling with LDA\n",
    "- [ ] Use `LatentDirichletAllocation` to extract topics from character lyrics.\n",
    "- [ ] Print top words per topic and assign topic distributions to characters.\n",
    "- [ ] Compare which themes dominate each characterâ€™s dialogue.\n",
    "\n",
    "> **Why it's interesting:** Surfaces hidden thematic structures â€” e.g., Hamilton might have â€œlegacy/politicsâ€ topics vs. Elizaâ€™s â€œfamily/love.â€\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Word Embedding Averages\n",
    "- [ ] Train Word2Vec or use pretrained GloVe vectors.\n",
    "- [ ] Average each characterâ€™s line embeddings.\n",
    "- [ ] Plot with PCA or t-SNE to visualize how semantically distinct they are.\n",
    "\n",
    "> **Why it's interesting:** Goes beyond word frequency â€” characters who *mean* similar things may show up close even if they use different vocab.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Emotion & Sentiment Analysis\n",
    "- [ ] Use NRC Emotion Lexicon or VADER to score lines for emotion categories.\n",
    "- [ ] Aggregate per character (e.g., Hamilton has more anger, Eliza more trust?)\n",
    "- [ ] Optionally visualize emotions over time or by act/song.\n",
    "\n",
    "> **Why it's interesting:** You can actually map Hamiltonâ€™s emotional arc â€” and compare it to Burrâ€™s or Angelicaâ€™s.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Syntactic & Stylistic Analysis\n",
    "- [ ] Use `spaCy` to extract POS tag distributions per character.\n",
    "- [ ] Analyze sentence length, use of exclamations/questions/imperatives.\n",
    "- [ ] Compare rhetoric: does Hamilton use more first-person pronouns? Does Burr ask more questions?\n",
    "\n",
    "> **Why it's interesting:** Stylometry insights â€” helps detect *how* characters speak, not just what they say.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤– Part 2: Machine Learning Classifier (Who Said This Line?)\n",
    "\n",
    "### 1. Create Dataset\n",
    "- [ ] Convert your extracted data into line/label pairs: `(line, speaker)`\n",
    "- [ ] Remove characters with very few lines or group them as \"OTHER\"\n",
    "- [ ] Split into training/test sets\n",
    "\n",
    "> **Why it's interesting:** Prepping your own dataset is the first step in any applied ML project â€” and youâ€™ll spot quirks in the dialogue.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Baseline Classifier with TF-IDF\n",
    "- [ ] Use `TfidfVectorizer` + `LogisticRegression`\n",
    "- [ ] Train model to predict the speaker\n",
    "- [ ] Evaluate accuracy and confusion matrix\n",
    "\n",
    "> **Why it's interesting:** This shows how distinguishable character voices are just by word use â€” are Elizaâ€™s lines harder to separate than Burrâ€™s?\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Try Other Classical Models\n",
    "- [ ] Swap in `MultinomialNB`, `RandomForestClassifier`, `SVM`\n",
    "- [ ] Compare performance\n",
    "\n",
    "> **Why it's interesting:** See which models handle sparse, high-dimensional text best â€” useful ML comparison exercise.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Neural Model (LSTM or BiLSTM)\n",
    "- [ ] Use `Tokenizer` + `pad_sequences` to prepare input\n",
    "- [ ] Train an LSTM-based classifier using Keras or PyTorch\n",
    "- [ ] Track training loss + accuracy\n",
    "\n",
    "> **Why it's interesting:** Neural models can â€œlearnâ€ writing style and sentence structure â€” deeper representation of how characters speak.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Transformer-Based Model (e.g. BERT)\n",
    "- [ ] Use HuggingFace `transformers` to fine-tune `bert-base-uncased`\n",
    "- [ ] Frame it as a text classification task (line â†’ character)\n",
    "- [ ] Evaluate results and compare to earlier models\n",
    "\n",
    "> **Why it's interesting:** Youâ€™re using state-of-the-art tools on a creative dataset â€” it's a strong portfolio piece.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Model Explainability\n",
    "- [ ] Try `LIME` or `SHAP` to explain why the model predicted a certain character\n",
    "- [ ] Visualize important words per prediction\n",
    "\n",
    "> **Why it's interesting:** Makes the model feel less like a black box â€” great way to show what distinguishes characters linguistically.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. (Optional) Interactive Demo\n",
    "- [ ] Use `Streamlit` or `Gradio` to build a web interface\n",
    "- [ ] User inputs a line, app predicts speaker + confidence + top keywords\n",
    "- [ ] Add character stats or emotion radar plots\n",
    "\n",
    "> **Why it's interesting:** Super fun way to present your work â€” and lets others play with your model.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŸ Bonus / Stretch Ideas\n",
    "\n",
    "- Compare Hamilton to *In The Heights* or *Les Mis* using the same pipeline\n",
    "- Cluster lines into emotion or topic types *regardless* of speaker\n",
    "- Detect sarcasm or rhetorical style\n",
    "- Analyze rhyme/meter patterns with phoneme tools (`pronouncing`, `textstat`)\n",
    "- Animate character emotion arcs across songs (timeline style)\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
